{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0afa4b-c0ea-4c20-9b39-cc36b9678d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Krauler():\n",
    "  def __init__(self, start_url, psubd):\n",
    "    self.__subd = psubd #один объект Bd_rabota на один объект краулера. типа это пользователь. не нужно беспокоиться о конкурентном доступе при многопоточной работе краулеров. СУБД сама разрулит одновременный доступ от разных краулеров. пул коннектов решил не делать,т.к. поток всегда чтото пишет.вроде логично держать один коннект на один поток/воркер\n",
    "    self.__krregno = self.__subd.day_svoy_nomer()\n",
    "    self.__subd.add_log(self.__class__.__name__,'init',1,'инициализация объекта Krauler.')\n",
    "\n",
    "#    try:\n",
    "#      self.__subd.add_log(self.__class__.__name__,'start1',1,f'Начало метода START1 в Krauler. стартую с урла:{self.__start_url}')\n",
    "#    except Log_20000:\n",
    "#      return 20000\n",
    "\n",
    "    self.__start_url = start_url  # откуда начинаем\n",
    "    self.__max_count = 1   # максимальное количество скачиваний урлов. сквозная относительно вложенности и количества урлов на странице\n",
    "    self.__now_count = 0   # текущее количество скачиваний урлов. сквозная относительно вложенности и количества урлов на странице\n",
    "    #self.__visited = set()          # посещенные страницы\n",
    "    self.__setup_lim_recurs = 10 #глубина вложенности. на сколько в рекурсию можем провалиться одним воркером\n",
    "    self.__now_recurs = 0 #глубина вложенности. на текущий момент\n",
    "    self.__setup_lim_href = 40 #максимальное количество ссылок на странице. сверх этого не обрабатываем\n",
    "    self.__setup_lim_size = 1048576 #максимальный размер ответа в байтах, который обрабатываем\n",
    "    self.__urldubley = 0 #здесь можно считать дубли, которые будут пытаться записываться в базу. для дальнейшего анализа\n",
    "\n",
    "  def start1(self): # по-большому счету для красоты, чтобы понять, откуда стартуем\n",
    "    self.__subd.add_log(self.__class__.__name__,'start1',1,f'Начало метода START1 в Krauler. стартую с урла:{self.__start_url}')\n",
    "    kod_dl = self.__download_link(self.__start_url, 1)\n",
    "    self.__subd.add_log(self.__class__.__name__,'start1',1,f'Закончил метод START1 в Krauler с кодом {kod_dl}, дублей всего было {self.__urldubley}')\n",
    "\n",
    "  def __download_link(self, url, status=0): # это уже рекуррентная функция\n",
    "    \"\"\"\n",
    "   if (self.__now_count>8):\n",
    "      print ('ddddddddddddddddddddddddddddddddddddddddd')\n",
    "      raise NNN\n",
    "    \"\"\"\n",
    "\n",
    "    if self.__now_recurs >= self.__setup_lim_recurs:\n",
    "      print (f'    {datetime.now()}| cls_kr.DOWNLOAD_LINK(idO={self.__krregno}): достигли лимит по кол-ву рекурсий выходим.')\n",
    "      self.__subd.add_log(self.__class__.__name__,'__download_link',30,f'--- начало, url={url}, status={status}, лимит(рекурс)={self.__now_recurs}')\n",
    "      return 'хватит по уровню рекурсии'\n",
    "    self.__subd.add_log(self.__class__.__name__,'__download_link',1,f'--- начало, url={url}, status={status}')\n",
    "    isklu4enie = None #моя личная классификация исключений. один код может включать несколько реальных кодов ошибок. так что это группировочные некие коды\n",
    "    errtxt = '' #для записи в БД текст ошибки. он будет появляться только если сработал эксепшн.\n",
    "\n",
    "    def get_redirect_url(rr): #это для возвращения урла, на который идет редирект\n",
    "      if rr.status_code in [301, 302, 303, 307, 308]:\n",
    "        return rr.headers.get('location') #регистронезависимое извлечение заголовка\n",
    "      return ''\n",
    "\n",
    "    try:\n",
    "      self.__now_count += 1 #попытка увеличивается независимо от того, будет ли гет успешным. может ссылка живая, но заблокирована по законодательству. получим таймаут или еще чего, но попытка увеличивается\n",
    "      response = requests.get(url,timeout=(3, 10), allow_redirects=False)  # 3 сек. на подключение, 10 сек. на чтение, запрет редиректов, иначе не разобраться\n",
    "      isklu4enie = 0   #если эксепшенов не было, то поставим в ноль. будет означать, что какойто код у нас имеется\n",
    "      response.raise_for_status() # Метод raise_for_status() выбросит исключение HTTPError, если статус ответа указывает на ошибку (4xx или 5xx):\n",
    "      kod_otveta=response.status_code\n",
    "      self.__subd.add_log(self.__class__.__name__,'__download_link',10,f' get(url) выполнен для: {url}. код ответа={kod_otveta}, всего посетили страниц: {self.__now_count}')\n",
    "\n",
    "      kod_insert = self.__subd.ins_url(url, get_redirect_url(response), isklu4enie, kod_otveta, status, self.__setup_lim_recurs,self.__setup_lim_href,self.__setup_lim_size, len(response.content),errtxt,self.__now_recurs) #запись в бд\n",
    "      if kod_insert == errorcode.ER_DUP_ENTRY:\n",
    "        self.__urldubley += 1\n",
    "        self.__subd.add_log(self.__class__.__name__,'__download_link',20,f'обнаружена попытка задублировать строку. похоже, что ее мы уже обрабатывали раньше. выходим.')\n",
    "        return 'дубль'\n",
    "      elif self.__now_count >= self.__max_count:\n",
    "        self.__subd.add_log(self.__class__.__name__,'__download_link',30,f'Достигли предел настроек по общему количестсву: max={self.__max_count}, текущий счетчик: {self.__now_count}')\n",
    "        return 'достигнут общий лимит'\n",
    "      if kod_otveta == 200:\n",
    "        self.__subd.add_log(self.__class__.__name__,'__download_link',40,f'обрабатываю ответ 200, буду посылать в парсер')\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        self.__parse_links(soup, url)\n",
    "      else:\n",
    "        self.__subd.add_log(self.__class__.__name__,'__download_link',50,f'Парсить не будем, т.к. не получил код 200: {url} (Status code: {kod_otveta})')\n",
    "\n",
    "    except requests.exceptions.ConnectTimeout as e:\n",
    "      isklu4enie = 1\n",
    "      errtxt = e\n",
    "    except requests.exceptions.ReadTimeout as e:\n",
    "      isklu4enie = 2\n",
    "      errtxt = e\n",
    "    except requests.exceptions.Timeout as e:\n",
    "      isklu4enie = 3\n",
    "      errtxt = e\n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "      isklu4enie = 1000\n",
    "      errtxt = e\n",
    "    except HTTPError as e:\n",
    "      isklu4enie = 404\n",
    "      errtxt = e\n",
    "    except requests.RequestException as e: #если скачать страничку не получилось. ошибка вобще непонятная при получении request\n",
    "      isklu4enie = 999\n",
    "      errtxt = e\n",
    "      print(f\"  {datetime.now()}| cls_kr.DOWNLOAD_LINK(idO={self.__krregno}): ошибка 999 при попытке сделать GET непонятная: {e}\")\n",
    "    except Exception as e: #если ошибка вобще-вобще непонятная\n",
    "      isklu4enie = 9999\n",
    "      errtxt = e\n",
    "      print(f\"  {datetime.now()}| cls_kr.DOWNLOAD_LINK(idO={self.__krregno}): ошибка 9999 вобще-вобще непонятная: {e} nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn\")\n",
    "    \n",
    "    if isklu4enie == None: # вобще такого никогда не должно быть\n",
    "      print(f'  {datetime.now()}| cls_kr.DOWNLOAD_LINK(idO={self.__krregno}): вобще хрень какая-то, не должно такого быть. на выходе.')\n",
    "      return 'error_HZ'\n",
    "    elif isklu4enie == 0:\n",
    "      return 'ok'  # если ноль, то значит мы сделали гет и он прошел норм и мы уже адрес записали в бд и сходили по рекурсии парсить\n",
    "    else: #здесь формируем инсерт в том случае, если было какое-то исключение.\n",
    "      kod_insert = self.__subd.ins_url(url, '', isklu4enie, 'NULL', status, self.__setup_lim_recurs,self.__setup_lim_href,self.__setup_lim_size, 0, errtxt,self.__now_recurs) #запись в бд\n",
    "    #except Exception: #если скачать страничку не получилось вобще непонятно почему\n",
    "      #print(f\"  cls_kr.DOWNLOAD_LINK(idO={self.__krregno}): ошибка вобще ХЗ --{Exception.error}\")\n",
    "      #print(f\"    cls_kr.DOWNLOAD_LINK(idO={self.__krregno}): переданный урл:{url}\")\n",
    "        # Delay to avoid overwhelming the server\n",
    "    time.sleep(1)\n",
    "\n",
    "  def __parse_links(self, soup, base_url):\n",
    "    print(f'{datetime.now()}| cls_kr.PARSE_LINKS(idO={self.__krregno}): зашел')\n",
    "    self.__now_recurs += 1\n",
    "    for link in soup.find_all('a', href=True):\n",
    "      absolute_url = urljoin(base_url, link['href']) #здесь могут быть еще параметры всякие в урле, нужно их выкусить\n",
    "      p_url = urlsplit(absolute_url)\n",
    "      absolute_url = urlunsplit((p_url.scheme, p_url.netloc, p_url.path, '', '')) #вот тут все выкусили из урла, все параметры и их значения. может и потом нужно их обрабатывать будет\n",
    "      #if absolute_url not in self.__visited:\n",
    "        #self.__download_link(absolute_url) # уходим в рекурсию\n",
    "      #print('абсолютный urllllllll:',absolute_url,', base_url=',base_url, '. сам href=', link['href'])\n",
    "      print (f\"  {datetime.now()}| cls_kr.PARSE_LINKS(idO={self.__krregno})| base_url={base_url}: нашел такой:{absolute_url}, link={link['href']}\")\n",
    "      kod_rec = self.__download_link(absolute_url) # уходим в рекурсию\n",
    "    print (f\"  {datetime.now()}| cls_kr.PARSE_LINKS(idO={self.__krregno})| base_url={base_url}: вышел из рекурсии:{absolute_url} с кодом выхода {kod_rec}, сейчас буду брать след. адрес\")\n",
    "    self.__now_recurs -= 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
